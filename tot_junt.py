# -*- coding: utf-8 -*-
"""tot_junt.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dJTQWPRB6nnFc-tgmxSXY9zPVTAAhIm7

#EXTRACCIÓN DE PERSONAJES Y SU CARACTERIZACIÓN
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install gutenbergpy
!pip install nltk
!pip install textblob

import requests
import pandas as pd
import nltk
import matplotlib.pyplot as plt
import gutenbergpy.textget
import os
import spacy
import re
import seaborn as sns

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.stem import WordNetLemmatizer
from nltk import pos_tag
from textblob import TextBlob
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from collections import Counter
from spacy import displacy
from gutenbergpy.gutenbergcache import GutenbergCache


nltk.download('gutenberg')
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')

nlp = spacy.load("en_core_web_sm")

def get_book(book_id):
    raw_book = gutenbergpy.textget.get_text_by_id(book_id)
    return raw_book.decode().strip()

book_id = {
    "The Wonderful Wizard of Oz": 55,
    "Grimm's Fairytales": 2591,
    "The Sociable Sand Witch": 68352
}

book_data = []
for title, id in book_id.items():
    text = get_book(id)
    book_data.append({'title': title, 'text': text})

df = pd.DataFrame(book_data)

def clean_text(text):
  text = text.lower()
  return text


df['clean_text'] = df['text'].apply(clean_text)

print(df[['title', 'clean_text']].head())

def count_tokens(text):
    doc = nlp(text)
    num_tokens = len([token for token in doc if not token.is_space])
    return num_tokens

df['num_tokens'] = df['clean_text'].apply(count_tokens) # Apply the function to the 'clean_text' column of the DataFrame and store the results in the new column 'num_tokens'.

num_tokens_total = df['num_tokens'].sum()
print(f"\nNúmero total de tokens: {num_tokens_total}")

num_tokens_fila_0 = df['num_tokens'].iloc[0]
print(f"\nNúmero de tokens de 'The Wonderful Wizard of Oz': {num_tokens_fila_0}")

num_tokens_fila_1 = df['num_tokens'].iloc[1]
print(f"\nNúmero de tokens de 'Grimm's Fairy Tales': {num_tokens_fila_1}")

num_tokens_fila_2 = df['num_tokens'].iloc[2]
print(f"\nNúmero de tokens de 'The Sociable Sand Witch': {num_tokens_fila_2}")

"""### LIMPIAR TEXTO
(contiene stopwords y se mantienen las mayúsculas)
"""

def remove_newlines(text):
    return text.replace("\n", " ")

def clean_text(text):
    stop_words = set(stopwords.words('english'))
    lemmatizer = WordNetLemmatizer()
    tokens = word_tokenize(text)
    cleaned_tokens = [lemmatizer.lemmatize(token.lower()) for token in tokens if token.isalpha() and token.lower() not in stop_words]
    return ' '.join(cleaned_tokens)

df['clean_text'] = df['text'].apply(clean_text)

print(df[['title', 'clean_text']].head())

def get_sentences(text):
    return sent_tokenize(text)



data = []

bookd_id = {
    "The Wonderful Wizard of Oz": 55,
    "Grimm's Fairytales": 2591,
    "The Sociable Sand Witch": 68352
}

# Iterate over the correct dictionary 'bookd_id'
for title, gutenberg_id in bookd_id.items():
    try:
        text = get_book(gutenberg_id)
        if text:
            data.append((title, text))
        else:
            print(f"No se pudo obtener el texto del libro {title} (ID {gutenberg_id})")
    except Exception as e:
        print(f"Error al descargar el libro {title} (ID {gutenberg_id}): {e}")

df = pd.DataFrame(data, columns=['title', 'text'])

output_path = '/content/drive/MyDrive/text_files/dataset.csv'

os.makedirs(os.path.dirname(output_path), exist_ok=True)

df.to_csv(output_path, index=False, encoding='utf-8')

df.head()

"""####INFO DATASET"""

def average_sentence_length(text):
    sentences = sent_tokenize(text)
    return sum(len(sentence.split()) for sentence in sentences) / len(sentences) if sentences else 0

df['avg_sentence_length'] = df['text'].apply(average_sentence_length)

df[['title', 'avg_sentence_length']].head()

def word_frequency(text, word):
    words = text.lower().split()
    word_count = words.count(word)
    total_words = len(words)
    return word_count / total_words if total_words > 0 else 0

print(word_frequency(text, 'witch'))

def count_word_occurrences(text, word):
    words = text.lower().split()
    return words.count(word)

print(count_word_occurrences(text, 'witch'))

"""##MAGO DE OZ

"""

def analyze_characters_oz(text):
    doc = nlp(text)
    characters = [ent.text for ent in doc.ents if ent.label_ == "PERSON" and "ORG"]
    character_counts = Counter(characters)
    return character_counts

index_oz = df[df['title'] == "The Wonderful Wizard of Oz"].index[0]

print(analyze_characters_oz(df['text'][index_oz]))

texto = df['text'][index_oz]

doc = nlp(texto)

entidades = [(ent.text, ent.label_) for ent in doc.ents]


adjetivos_entidades = {}
for token in doc:
    if token.pos_ == "ADJ":
        for ent in doc.ents:
            if token.head == ent.root or token.head.head == ent.root:
                if ent.text in adjetivos_entidades:
                    adjetivos_entidades[ent.text].append(token.text)
                else:
                    adjetivos_entidades[ent.text] = [token.text]

rows = []
for entidad, adjetivos in adjetivos_entidades.items():
    entidad_tipo = next(ent.label_ for ent in doc.ents if ent.text == entidad)
    rows.append((entidad, entidad_tipo, ', '.join(adjetivos)))

df_wizard_oz = pd.DataFrame(rows, columns=['Entidad', 'Tipo', 'Adjetivos'])

csv_filename = "/content/drive/MyDrive/text_files//entidades_adjetivos_todos.csv"
df.to_csv(csv_filename, index=False)

def generate_adjectives_table(text, characters):
    doc = nlp(text)
    character_adjectives = {}
    adjetivos_entidades = {}
    for token in doc:
        if token.pos_ == "ADJ":
            for ent in doc.ents:
                if token.head == ent.root or token.head.head == ent.root:
                    if ent.text in adjetivos_entidades:
                        adjetivos_entidades[ent.text].append(token.text)
                    else:
                        adjetivos_entidades[ent.text] = [token.text]

    rows = []
    for entidad, adjetivos in adjetivos_entidades.items():
        entidad_tipo = next(ent.label_ for ent in doc.ents if ent.text == entidad)
    rows.append((entidad, entidad_tipo, ', '.join(adjetivos)))

    character_df = pd.DataFrame([
        {"Personaje": personaje, "Adjetivo": ", ".join(adjetivos)}
        for personaje, adjetivos in character_adjectives.items()
    ])

    return character_df

index_oz = df[df['title'] == "The Wonderful Wizard of Oz"].index[0]
text_oz = df['text'][index_oz]

characters = analyze_characters_oz(text_oz)

result_df = generate_adjectives_table(text_oz, characters)

print(result_df)

data = []
for title, gutenberg_id in book_id.items():
    text = get_book(55)

    # Calcular la longitud media de las frases
    avg_sentence_len = average_sentence_length(text)

    # Calcular la frecuencia de la palabra "bruja"
    bruja_freq = word_frequency(text, 'witch')
    bruja_count = count_word_occurrences(text, 'witch')
    total_words = len(text.split())
    bruja_freq_percentage = bruja_freq * 100

    # Análisis de personajes
    character_counts = analyze_characters_oz(text)
    top_characters = character_counts.most_common(10)  # Top 10 personajes

    # Guardar los resultados
    data.append({
        'title': title,
        'avg_sentence_length': avg_sentence_len,
        'bruja_count': bruja_count,
        'total_words': total_words,
        'bruja_frequency_percentage': bruja_freq_percentage,
        'top_characters': top_characters
    })

print(data)

freq_df = pd.DataFrame(data)

# Calcular la desviación estándar de la frecuencia de "bruja"
bruja_std_percentage = freq_df['bruja_frequency_percentage'].std()

# Mostrar los resultados
print(f"Desviación estándar de la frecuencia de 'bruja' (como porcentaje): {bruja_std_percentage:.6f}")
print(freq_df[['title', 'avg_sentence_length', 'bruja_count', 'total_words', 'bruja_frequency_percentage', 'top_characters']])

def find_witch_sentences_with_context_oz(text, context_words=3):
    witch_sentences = []
    sentences = get_sentences(text)
    for sentence in sentences:
        if 'witch' in sentence.lower():
            words = nltk.word_tokenize(sentence)
            witch_indices = [i for i, word in enumerate(words) if word.lower() == 'witch']
            for index in witch_indices:
                start = max(index - context_words, 0)
                end = min(index + context_words + 1, len(words))
                context = ' '.join(words[start:end])
                witch_sentences.append(context)
    return witch_sentences


context_words = 3
witch_sentences_with_context = find_witch_sentences_with_context_oz(text, context_words)

witch_sentences_df = pd.DataFrame(witch_sentences_with_context, columns=['sentence'])
print(witch_sentences_df)

def find_wizard_with_context(text, context_words=3):
    wizard_sentences = []
    sentences = get_sentences(text)
    for sentence in sentences:
        if 'wizard' in sentence.lower():
            words = nltk.word_tokenize(sentence)
            wizard_indices = [i for i, word in enumerate(words) if word.lower() == 'wizard']
            for index in wizard_indices:
                start = max(index - context_words, 0)
                end = min(index + context_words + 1, len(words))
                context = ' '.join(words[start:end])
                wizard_sentences.append(context)
    return wizard_sentences

context_words = 3
wizard_sentences_with_context = find_wizard_with_context(text, context_words)

wizard_sentences_df = pd.DataFrame(wizard_sentences_with_context, columns=['sentence'])
print(wizard_sentences_df)

# prompt: run sentiment analysis comparing witch and wizard sentences on a piled bar

import matplotlib.pyplot as plt
from textblob import TextBlob

def analyze_sentiment(text):
    blob = TextBlob(text)
    return blob.sentiment.polarity
witch_sentences_df['sentiment'] = witch_sentences_df['sentence'].apply(analyze_sentiment)
wizard_sentences_df['sentiment'] = wizard_sentences_df['sentence'].apply(analyze_sentiment)
witch_sentiment_mean = witch_sentences_df['sentiment'].mean()
wizard_sentiment_mean = wizard_sentences_df['sentiment'].mean()
plt.figure(figsize=(8, 6))
plt.bar(['Witch', 'Wizard'], [witch_sentiment_mean, wizard_sentiment_mean], color=['cornflowerblue', 'darkslategrey'])
plt.title('Sentiment Analysis of Witch vs. Wizard Sentences')
plt.ylabel('Average Sentiment Polarity')

plt.ylim(-1,1)

plt.show()

def extract_adjectives_from_sentences(sentences):
    result = []
    for sentence in sentences:
        words = word_tokenize(sentence)
        tagged_words = nltk.pos_tag(words)
        for word, tag in tagged_words:
            if tag == 'JJ':  # Adjetivo
                result.append(word.lower())
    return result

def analyze_adjective_sentiment(adjectives):
    sentiment_scores = []
    for adjective in adjectives:
        blob = TextBlob(adjective)
        score = blob.sentiment.polarity
        sentiment_scores.append((adjective, score))
    return sentiment_scores

results = {}

book_ids = {
    "The Wonderful Wizard of Oz": 55,
    "Grimm's Fairytales": 2591,
    "The Sociable Sand Witch": 59645
}

for title, gutenberg_id in book_id.items():
    # Obtener el texto del libro desde Proyecto Gutenberg
    raw_text = get_book(55)

    # Primero, busca las frases con "witch" en el texto original sin limpiar
    witch_sentences_with_context = find_witch_sentences_with_context_oz(raw_text)

    # Ahora, extrae los adjetivos de esas frases
    witch_adjectives = extract_adjectives_from_sentences(witch_sentences_with_context)

    if witch_adjectives:
        # Calcular la frecuencia de los adjetivos
        adjective_frequencies = pd.Series(witch_adjectives).value_counts().reset_index()
        adjective_frequencies.columns = ['adjective', 'frequency']

        # Analizar el sentimiento de los adjetivos
        adjective_sentiments = analyze_adjective_sentiment(witch_adjectives)

        # Mostrar los resultados para cada libro
        print(f"\nResultados para el libro: {title}")
        print(f"Frases con 'witch' y contexto:")
        print(witch_sentences_with_context)

        print(f"\nFrecuencia de adjetivos:")
        print(adjective_frequencies.head())

        print(f"\nSentimiento de adjetivos:")
        for adjective, score in adjective_sentiments:
            print(f"{adjective}: {score}")

    else:
        print(f"\nNo se encontraron adjetivos relacionados con 'witch' en el libro: {title}")

"""## MAGO DE OZ (2.0)"""

def find_witch_sentences_with_word_context(text, context_words=7):
    sentences = sent_tokenize(text)
    result = []
    for sentence in sentences:
        if 'witch' in sentence:
            words = word_tokenize(sentence)
            witch_indices = [i for i, word in enumerate(words) if word == 'witch']
            for index in witch_indices:
                start = max(index - context_words + 1, 0)
                end = min(index + context_words + 1, len(words))
                context = ' '.join(words[start:end])
                result.append(context)
    return result

print(find_witch_sentences_with_word_context(text))

def find_witch_sentences_with_word_context(text, word="witch"):
  sentences = re.findall(r'([^.]*? ' + re.escape(word) + r' [^.]*\.)', text, re.IGNORECASE)
  return sentences

print(find_witch_sentences_with_word_context(text))

sentences_with_witch = find_witch_sentences_with_word_context(text)

# Mostrar en una tabla con pandas
sentences_with_witch_oz_df = pd.DataFrame(sentences_with_witch, columns=["Sentence"])

# Guardar en un fichero CSV
sentences_with_witch_oz_df.to_csv("witch_sentences.csv", index=False)

def extract_adjectives_from_sentences(sentences):
  adjectives = []
  for sentence in sentences:
      doc = nlp(sentence)
      for token in doc:
          if token.text.lower() == "witch":
            head_token = token.head
            if head_token.pos_ == "ADJ":
                adjectives.append(child.text)
            for child in token.children:
              if child.pos == "ADJ":
                adjectives.append(child.text)
            for child in head_token.children:
              if child.pos_ == "ADJ" and child.text.lower () != "witch":
                adjectives.append(child.text)
  return adjectives

for title, gutenberg_id in book_ids.items():
    raw_text = get_book(gutenberg_id)

    witch_sentences_with_context = find_witch_sentences_with_word_context(raw_text)

    witch_adjectives = extract_adjectives_from_sentences(witch_sentences_with_context)

    if witch_adjectives:
        adjective_frequencies = pd.Series(witch_adjectives).value_counts().reset_index()
        adjective_frequencies.columns = ['adjective', 'frequency']

        adjective_sentiments = analyze_adjective_sentiment(witch_adjectives)

        print(f"\nResultados para el libro: {title}")
        print(f"Frases con 'witch' y contexto:")
        print(witch_sentences_with_context)

        print(f"\nFrecuencia de adjetivos:")
        print(adjective_frequencies.head())

        print(f"\nSentimiento de adjetivos:")
        for adjective, score in adjective_sentiments:
          print(f"{adjective}: {score}")

    else:
        print(f"\nNo se encontraron adjetivos relacionados con 'witch' en el libro: {title}")

"""## MAGO DE OZ 3.0

"""

texto = df['text'][index_oz]

doc = nlp(texto)


entidades = [(ent.text, ent.label_) for ent in doc.ents]

adjetivos_entidades = {}
for ent in doc.ents:
    adjetivos = []
    # Recorrer todos los tokens del documento
    for token in doc:
        # Si el token es un adjetivo, buscamos relaciones indirectas o amplias
        if token.pos_ == "ADJ":
            # Comprobamos si está relacionado con la entidad o con un sustantivo cercano
            if token.head == ent.root or token.head.head == ent.root or token.head in ent.root.subtree:
                adjetivos.append(token.text)
            # Considerar adjetivos dentro de las oraciones subordinadas
            elif ent.start < token.i < ent.end and token.dep_ in ("amod", "acomp", "advmod"):
                adjetivos.append(token.text)
    if adjetivos:
        adjetivos_entidades[ent.text] = adjetivos

rows = []
for entidad, adjetivos in adjetivos_entidades.items():
    entidad_tipo = next(ent.label_ for ent in doc.ents if ent.text == entidad)
    rows.append((entidad, entidad_tipo, ', '.join(adjetivos)))


wizard_df = pd.DataFrame(rows, columns=['Entidad', 'Tipo', 'Adjetivos'])

csv_filename = "/content/drive/MyDrive/entidades_adjetivos_spacy.csv"
wizard_df.to_csv(csv_filename, index=False)

entidades_interes = ["Glinda", "Wicked Witch", "Witch", "North Witch", "Wizard"]

entidades_relevantes = [ent for ent in doc.ents if any(e in ent.text for e in entidades_interes)]

adjetivos_entidades = {}
for ent in entidades_relevantes:
    adjetivos = []
    for token in doc:
        if token.pos_ == "ADJ":
            if token.head == ent.root or token.head.head == ent.root or token.head in ent.root.subtree:
                adjetivos.append(token.text)
    if adjetivos:
        adjetivos_entidades[ent.text] = adjetivos

# Crear lista para el DataFrame con múltiples adjetivos en una sola celda
rows = []
for entidad, adjetivos in adjetivos_entidades.items():
    entidad_tipo = next(ent.label_ for ent in doc.ents if ent.text == entidad)
    rows.append((entidad, entidad_tipo, ', '.join(adjetivos)))  # Unir adjetivos en una celda

# Crear DataFrame usando pandas
wizard_df = pd.DataFrame(rows, columns=['Entidad', 'Tipo', 'Adjetivos'])

# Guardar el DataFrame en un archivo CSV
csv_filename = "/content/drive/MyDrive/entidades_adjetivos_filtrados.csv"
wizard_df.to_csv(csv_filename, index=False)

patron_bruja = re.compile(r"\b(Witch|Wicked Witch|Good Witch|North Witch)\b", re.IGNORECASE)

frases_bruja = []
for sent in doc.sents:
    if re.search(patron_bruja, sent.text):
        frases_bruja.append(sent.text)


df_frases_bruja = pd.DataFrame(frases_bruja, columns=["Frase"])

# Mostrar el DataFrame con las frases
print(df_frases_bruja)

# Ejecutar dependency parsing en cada frase que menciona a la bruja
for i, frase in enumerate(frases_bruja):
    print(f"\nDependency parsing para la frase {i+1}: {frase}")
    doc_frase = nlp(frase)
    for token in doc_frase:
        print(f"{token.text} ({token.dep_}) <-- {token.head.text}")

json_filename = "/content/drive/MyDrive/frases_bruja.json"
df_frases_bruja.to_json(json_filename, index=False)

output_dir = "/content/drive/MyDrive/TFM"

frases_bruja_oz = [
  "'But I thought all witches were wicked' said the girl, who was half frightened at facing a real witch",
  "Those who dwelt in the East and the West were, indeed, wicked withces; but now that you have killed one of them, there is but one Wicked Withc in all the Land of Oz-the one who lives in the West",
  "'Oz himself is the Great Wizard', answered the Witch, sinking her voice to a whisper",
  "that country, where the Winkies live, is ruled by the Wicked Witch of the West, who would make you her salve if you passed her way",
  "The Witch gave Dorothy a friendly little nod, whirled around on her left heel three times, and straightaway disappeared, much to the surprise of little Toto, who barked after her loudly enough when se had gone, because he had been afraid even to growl while she stood by",
  "One in a while she would pass a house, and the people came out to look at her and bow low as she went by; for everyone knew she had been the means of destroying the Wicked Witch and setting them free from bondage",
  "The people greeted Dorothy kindly, and invited her to supper and to pass the night with them; for this was the home of one of the richest Munchkins in the land, and his friends were gathered with him to celebrate their freedom from the bondage of the Wicked Witch",
  "So we know you are a friendly witch",
  "'I thought I had beaten the Wicked Witch then, and I worked harder than ever; but I little knew how cruel my enemy could be'",
  "But I am not afraid so long as I have my oil-can, and nothing can hurt the Scarecrow, while you bear upon your forehead the mark of the Good Witch's kiss, and that will protect you from harm",
  "Remember that the Witch is Wicked-tremendously Wicked-and ought to be killed",
  "Now the Wicked Witch of the West had but one eye, yet that was as powerful as a telescope, and could see everywhere",
  "But she was a powerful Witch, as well as a wicked one"

]


for i, frase in enumerate(frases_bruja_oz):
    print(f"Guardando visualización de dependencias para la frase {i+1}: {frase}")
    doc_frase = nlp(frase)

    svg = displacy.render(doc_frase, style='dep', jupyter=False, options={'distance': 110})


    file_path = os.path.join(output_dir, f"frase_bruja_{i+1}.svg")

    with open(file_path, "w", encoding="utf-8") as file:
        file.write(svg)

def analyze_sentiment(text):
    blob = TextBlob(text)
    return blob.sentiment.polarity

sentiment_scores = []
for frase in frases_bruja_oz:
  sentiment = analyze_sentiment(frase)
  sentiment_scores.append(sentiment)

df_frases_bruja_oz = pd.DataFrame({'Frase': frases_bruja_oz, 'Sentimiento': sentiment_scores})
print(df_frases_bruja_oz)

"""## CUENTOS DE LOS HERMANOS GRIMM"""

def extraer_hausmarchen(text):
    patron_obra = r"1\.\s+Grimm's Fairy Tales"
    inicio_obra = re.search(patron_obra, text)

    if not inicio_obra:
        raise ValueError ("No se encontró")

    texto_hausmarchen = texto[inicio_obra.end():]

    return texto_hausmarchen

def dividir_libro_en_cuentos(text, titles):

    titles = [title.lower() for title in titles]

    lines = text.split('\n')
    stories = {}
    current_title = None
    current_story = []

    for line in lines:
      line_lower = line.strip().lower()
      if line_lower in titles:
        if current_title and current_story:
          stories[current_title] = ' '.join(current_story)
        current_title = line.strip()
        current_story = []
        print (f"Títulos encontrados: {line}")
      elif current_title:
        current_story.append(line.strip())

    if curent_title and current_story:
      stories[current_title] = ' '.join(current_story)

    return stories

titles = [
    "Jorinda and Jorindel",
    "Hansel and Gretel",
    "Mother Holle",
    "Little Red-Cap [Little Red Riding Hood]",
    "The Robber Bridegroom",
    "Sweetheart Roland",
    "The Juniper Tree",
    "The Pink",
    "The Raven"
]


texto_hausmarchen = extraer_hausmarchen(book_text_with_headers)

stories_with_headers = extract_stories_with_headers(texto_hausmarchen, titles)

for title, story in stories_with_headers.items():
  print(f"{title} - Longitud: {len(story)} caracteres")
  print (f"{story[:500]}...\n")

def analyze_characters_oz(text):
    doc = nlp(text)
    characters = [ent.text for ent in doc.ents if ent.label_ == "PERSON" and "ORG"]
    character_counts = Counter(characters)
    return character_counts

index_oz = df[df['title'] == "Grimm's Fairytales"].index[0]

print(analyze_characters_oz(df['text'][index_oz]))

texto = df['text'][1]

doc = nlp(texto)

entidades = [(ent.text, ent.label_) for ent in doc.ents]


adjetivos_entidades_grimm = {}
for token in doc:
    if token.pos_ == "ADJ":
        for ent in doc.ents:
            if token.head == ent.root or token.head.head == ent.root:
                if ent.text in adjetivos_entidades_grimm:
                    adjetivos_entidades_grimm[ent.text].append(token.text)
                else:
                    adjetivos_entidades_grimm[ent.text] = [token.text]

rows = []
for entidad, adjetivos in adjetivos_entidades_grimm.items():
    entidad_tipo = next(ent.label_ for ent in doc.ents if ent.text == entidad)
    rows.append((entidad, entidad_tipo, ', '.join(adjetivos)))

print(adjetivos_entidades_grimm)

grimm_df = pd.DataFrame(rows, columns=['Entidad', 'Tipo', 'Adjetivos'])

csv_filename = "/content/drive/MyDrive/text_files/adjetivos_entidades_grimm.csv"
df.to_csv(csv_filename, index=False)

entidades_interes_grimm = ["Mother Holle", "Witch", "Gothel", "Old lady"]

entidades_interes_grimm = [ent for ent in doc.ents if any(e in ent.text for e in entidades_interes_grimm)]

adjetivos_entidades_grimm = {}
for ent in entidades_interes_grimm:
    adjetivos = []
    for token in doc:
        if token.pos_ == "ADJ":
            if token.head == ent.root or token.head.head == ent.root or token.head in ent.root.subtree:
                adjetivos.append(token.text)
    if adjetivos:
        adjetivos_entidades_grimm[ent.text] = adjetivos

# Crear lista para el DataFrame con múltiples adjetivos en una sola celda
rows = []
for entidad, adjetivos in adjetivos_entidades_grimm.items():
    entidad_tipo = next(ent.label_ for ent in doc.ents if ent.text == entidad)
    rows.append((entidad, entidad_tipo, ', '.join(adjetivos)))  # Unir adjetivos en una celda

# Crear DataFrame usando pandas
grimm_df = pd.DataFrame(rows, columns=['Entidad', 'Tipo', 'Adjetivos'])

# Guardar el DataFrame en un archivo CSV
csv_filename = "/content/drive/MyDrive/entidades_adjetivos_filtrados.csv"
df.to_csv(csv_filename, index=False)

patron_bruja = re.compile(r"\b(Mother Holle|Witch|Gothel|North Witch|Woman)\b", re.IGNORECASE)

frases_bruja = []
for sent in doc.sents:
    if re.search(patron_bruja, sent.text):
        frases_bruja.append(sent.text)


df_frases_bruja = pd.DataFrame(frases_bruja, columns=["Frase"])

# Mostrar el DataFrame con las frases
print(df_frases_bruja)

# Ejecutar dependency parsing en cada frase que menciona a la bruja
for i, frase in enumerate(frases_bruja):
    print(f"\nDependency parsing para la frase {i+1}: {frase}")
    doc_frase = nlp(frase)
    for token in doc_frase:
        print(f"{token.text} ({token.dep_}) <-- {token.head.text}")

frases_bruja_grimm = [
    "At this the robber ran back as fast as he could to his comrades, and told the captain how a horrid witch had got into the house",
    "But the duck swam to her, seized her head in its beak and drew her into the water, and there the old witch had to drown.",
    "The old woman had only pretended to be so kind; she was in reality a wicked witch, who lay in wait for children, and had only built the little house of bread in order to entice them there.",
    "Gretel began to weep bitterly, but it was all in vain, for she was forced to do what the wicked witch commanded.",
    "then she began to howl quite horribly, but Gretel ran away and the godless witch was miserably burnt to death.",
    "'The old witch is dead!'",
    "Then the witch was very angry, and said, 'Such a cloak is very rare and wonderful thing, and I must and will have it.'",
    "But the old witch made a deep sleep come upon him, and he said to the young lady",
    "'You must be a witch, Cat-skin'; 'for you always put something into your soup, so that it pleases the king better than mine.'"
]


for i, frase in enumerate(frases_bruja_grimm):
    print(f"Guardando visualización de dependencias para la frase {i+1}: {frase}")
    doc_frase = nlp(frase)

    displacy.render(doc_frase, style='dep', jupyter=True, options={'distance': 110})

!pip install textblob==0.17.1

from textblob import TextBlob

def analyze_sentiment(frases_bruja_grimm):
    sentiment_scores = []
    for sentence in frases_bruja_grimm:
      blob = TextBlob(sentence)
      sentiment_scores.append(blob.sentiment.polarity)
    return sentiment_scores

print(analyze_sentiment(frases_bruja_grimm))

"""## THE SOCIABLE SAND WITCH"""

def analyze_characters_sandwitch(text):
    doc = nlp(text)
    characters = [ent.text for ent in doc.ents if ent.label_ == "PERSON" and "ORG"]
    character_counts = Counter(characters)
    return character_counts

index_sandwitch = df[df['title'] == "The Sociable Sand Witch"].index[0]

print(analyze_characters_sandwitch(df['text'][2]))

texto = df['text'][2]

doc = nlp(texto)

entidades = [(ent.text, ent.label_) for ent in doc.ents]


adjetivos_entidades_sandwitch = {}
for token in doc:
    if token.pos_ == "ADJ":
        for ent in doc.ents:
            if token.head == ent.root or token.head.head == ent.root:
                if ent.text in adjetivos_entidades_sandwitch:
                    adjetivos_entidades_sandwitch[ent.text].append(token.text)
                else:
                    adjetivos_entidades_sandwitch[ent.text] = [token.text]

rows = []
for entidad, adjetivos in adjetivos_entidades_sandwitch.items():
    entidad_tipo = next(ent.label_ for ent in doc.ents if ent.text == entidad)
    rows.append((entidad, entidad_tipo, ', '.join(adjetivos)))

print(adjetivos_entidades_sandwitch)

sandwitch_df = pd.DataFrame(rows, columns=['Entidad', 'Tipo', 'Adjetivos'])
print(sandwitch_df)

csv_filename = "/content/drive/MyDrive/text_files/adjetivos_entidades_sandwitch.csv"
sandwitch_df.to_csv(csv_filename, index=False)

entidades_interes_sandwitch = ["Sand Witch", "Witch"]

entidades_relevantes_sandwitch = [ent for ent in doc.ents if any(e in ent.text for e in entidades_interes)]

adjetivos_entidades_sandwitch = {}
for ent in entidades_relevantes_sandwitch:
    adjetivos = []
    for token in doc:
        if token.pos_ == "ADJ":
            if token.head == ent.root or token.head.head == ent.root or token.head in ent.root.subtree:
                adjetivos.append(token.text)
    if adjetivos:
        adjetivos_entidades_sandwitch[ent.text] = adjetivos

# Crear lista para el DataFrame con múltiples adjetivos en una sola celda
rows = []
for entidad, adjetivos in adjetivos_entidades_sandwitch.items():
    entidad_tipo = next(ent.label_ for ent in doc.ents if ent.text == entidad)
    rows.append((entidad, entidad_tipo, ', '.join(adjetivos)))  # Unir adjetivos en una celda

# Crear DataFrame usando pandas
sandwitch_df = pd.DataFrame(rows, columns=['Entidad', 'Tipo', 'Adjetivos'])

# Guardar el DataFrame en un archivo CSV
csv_filename = "/content/drive/MyDrive/entidades_adjetivos_filtrados.csv"
sandwitch_df.to_csv(csv_filename, index=False)

patron_bruja = re.compile(r"\b(Sand Witch|Witch)\b", re.IGNORECASE)

frases_sandwitch = []
for sent in doc.sents:
    if re.search(patron_bruja, sent.text):
        frases_sandwitch.append(sent.text)


df_frases_sandwitch = pd.DataFrame(frases_bruja, columns=["Frase"])

# Mostrar el DataFrame con las frases
print(df_frases_sandwitch)

# Ejecutar dependency parsing en cada frase que menciona a la bruja
for i, frase in enumerate(frases_sandwitch):
    print(f"\nDependency parsing para la frase {i+1}: {frase}")
    doc_frase = nlp(frase)
    for token in doc_frase:
        print(f"{token.text} ({token.dep_}) <-- {token.head.text}")

json_filename = "/content/drive/MyDrive/frases_sandwitch.json"
df_frases_sandwitch.to_json(json_filename, index=False)

output_dir = "/content/drive/MyDrive/TFM"

frases_sandwitch = [
    "Of all the witches that may be found in all the fairy tales ever told there is none more delightfully sociable than the Sand Witch",
    "'Not children like mine', said the Sand Witch, proudly",
    "If there's a more beautiful child than little Lettuce Sand Witch I'd like to see it.",
    "And as for dear little Ham Sand Witch, he is the cutest thing.",
    "And then it was that the Sand Witch showed what a thoroughly sociable nature she had, for although the boy had turned his back to her and was paying no attention, she wasn't in the least discouraged.",
    "This Witch, who lives underneath the heaps of sand at the ocean's edge, where, in the summertime, you dig your shovel, is not at all like other witches",
    "All the Sand Witch knows how to do is to sink into the sand when anything scares her, and to come up through the sand when she sees a chance to get acquainted with a person she never was acquainted with before",
    "The first Junior knew about the Sand Witch was when the tip end of a steeple hat began to come up through the sand in front of him",
    "Up, up it came until the whole hat was showing; then followed a long nose, two big, black eyes, a big mouth, and a sharp pointed chin; after that the rest of the Sand Witch followed very quickly, until at least she stood before him as cool as a cucumber."

]


for i, frase in enumerate(frases_sandwitch):
    print(f"Guardando visualización de dependencias para la frase {i+1}: {frase}")
    doc_frase = nlp(frase)

    svg = displacy.render(doc_frase, style='dep', jupyter=False, options={'distance': 110})


    file_path = os.path.join(output_dir, f"frase_bruja_{i+1}.svg")

    with open(file_path, "w", encoding="utf-8") as file:
        file.write(svg)

def analyze_sentiment(text):
    blob = TextBlob(text)
    return blob.sentiment.polarity

sentiment_scores = []
for frase in frases_sandwitch:
  sentiment = analyze_sentiment(frase)
  sentiment_scores.append(sentiment)

df_frases_sandwitch_sentiment = pd.DataFrame({'Frase': frases_sandwitch, 'Sentimiento': sentiment_scores})
print(df_frases_sandwitch_sentiment)